# download_small_model.py - T·∫£i model nh·ªè ph√π h·ª£p v·ªõi 7.5GB RAM
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

def download_small_models():
    """T·∫£i c√°c model nh·ªè ph√π h·ª£p v·ªõi RAM th·∫•p"""
    
    models_to_try = [
        {
            "name": "TinyLlama 1.1B", 
            "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "size_gb": 2.2,
            "description": "Model 1.1B params, r·∫•t nh·ªè, ph√π h·ª£p cho chat"
        },
        {
            "name": "Phi-2 2.7B",
            "model_id": "microsoft/phi-2", 
            "size_gb": 5.4,
            "description": "Model 2.7B params t·ª´ Microsoft, ch·∫•t l∆∞·ª£ng t·ªët"
        },
        {
            "name": "DialoGPT Medium",
            "model_id": "microsoft/DialoGPT-medium",
            "size_gb": 1.4,
            "description": "Model 355M params, chuy√™n chat, r·∫•t nh·∫π"
        }
    ]
    
    print("=== T·∫¢I MODEL NH·ªé CHO M√ÅY 7.5GB RAM ===\n")
    
    for i, model_info in enumerate(models_to_try, 1):
        print(f"{i}. {model_info['name']}")
        print(f"   Size: ~{model_info['size_gb']}GB")
        print(f"   M√¥ t·∫£: {model_info['description']}")
        print()
    
    choice = input("Ch·ªçn model ƒë·ªÉ t·∫£i (1-3) ho·∫∑c 'all' ƒë·ªÉ t·∫£i t·∫•t c·∫£: ").strip()
    
    if choice.lower() == 'all':
        selected_models = models_to_try
    else:
        try:
            idx = int(choice) - 1
            if 0 <= idx < len(models_to_try):
                selected_models = [models_to_try[idx]]
            else:
                print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
                return
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë h·ª£p l·ªá!")
            return
    
    for model_info in selected_models:
        print(f"\nüîÑ ƒêang t·∫£i {model_info['name']}...")
        
        try:
            model_id = model_info['model_id']
            save_path = f"models/small_model_{model_info['name'].lower().replace(' ', '_').replace('-', '_')}"
            
            # T·∫°o th∆∞ m·ª•c
            os.makedirs(save_path, exist_ok=True)
            
            # T·∫£i tokenizer
            print("  - ƒêang t·∫£i tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
            tokenizer.save_pretrained(save_path)
            
            # T·∫£i model
            print("  - ƒêang t·∫£i model...")
            model = AutoModelForCausalLM.from_pretrained(
                model_id, 
                trust_remote_code=True,
                torch_dtype="auto"
            )
            model.save_pretrained(save_path)
            
            print(f"  ‚úÖ ƒê√£ t·∫£i {model_info['name']} th√†nh c√¥ng!")
            print(f"  üìÅ L∆∞u t·∫°i: {save_path}")
            
            # T·∫°o file h∆∞·ªõng d·∫´n
            with open(f"{save_path}/README.txt", "w", encoding="utf-8") as f:
                f.write(f"Model: {model_info['name']}\n")
                f.write(f"Size: ~{model_info['size_gb']}GB\n")
                f.write(f"Description: {model_info['description']}\n")
                f.write(f"Original ID: {model_id}\n\n")
                f.write("ƒê·ªÉ s·ª≠ d·ª•ng model n√†y, s·ª≠a ƒë∆∞·ªùng d·∫´n trong config.yml:\n")
                f.write(f"models:\n")
                f.write(f"  llm_local_path: '{save_path}'\n")
            
        except Exception as e:
            print(f"  ‚ùå L·ªói khi t·∫£i {model_info['name']}: {e}")
    
    print(f"\nüéâ Ho√†n th√†nh! ƒê·ªÉ s·ª≠ d·ª•ng model m·ªõi:")
    print("1. M·ªü file 'config/config.yml'")
    print("2. S·ª≠a d√≤ng 'llm_local_path' th√†nh ƒë∆∞·ªùng d·∫´n model m·ªõi")
    print("3. Ch·∫°y l·∫°i 'python app.py'")

if __name__ == "__main__":
    download_small_models()