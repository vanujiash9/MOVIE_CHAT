# download_direct.py - T·∫£i model tr·ª±c ti·∫øp v√†o D:
import os
from transformers import AutoTokenizer, AutoModelForCausalLM

# Set cache v√†o D:
os.environ['HF_HOME'] = 'D:/huggingface_cache'
os.environ['TRANSFORMERS_CACHE'] = 'D:/huggingface_cache'

def download_small_model():
    """T·∫£i model nh·ªè tr·ª±c ti·∫øp v√†o D:"""
    
    model_id = "microsoft/DialoGPT-medium"  # Ch·ªâ 1.4GB
    save_path = "models/small_model_dialogpt"
    
    try:
        print("üîÑ ƒêang t·∫£i DialoGPT Medium (1.4GB)...")
        print("üìÅ Cache s·∫Ω l∆∞u v√†o D: drive")
        
        # T·∫°o th∆∞ m·ª•c
        os.makedirs(save_path, exist_ok=True)
        
        # T·∫£i tokenizer
        print("  - ƒêang t·∫£i tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        tokenizer.save_pretrained(save_path)
        
        # T·∫£i model
        print("  - ƒêang t·∫£i model...")
        model = AutoModelForCausalLM.from_pretrained(model_id)
        model.save_pretrained(save_path)
        
        print(f"  ‚úÖ ƒê√£ t·∫£i DialoGPT th√†nh c√¥ng!")
        print(f"  üìÅ L∆∞u t·∫°i: {save_path}")
        
        # T·∫°o file h∆∞·ªõng d·∫´n
        with open(f"{save_path}/README.txt", "w", encoding="utf-8") as f:
            f.write("Model: DialoGPT Medium\n")
            f.write("Size: ~1.4GB\n")
            f.write("Description: Model 355M params, chuy√™n chat, r·∫•t nh·∫π\n")
            f.write(f"Original ID: {model_id}\n\n")
            f.write("ƒê·ªÉ s·ª≠ d·ª•ng model n√†y, s·ª≠a ƒë∆∞·ªùng d·∫´n trong config.yml:\n")
            f.write(f"models:\n")
            f.write(f"  llm_local_path: '{save_path}'\n")
        
        print(f"\nüéâ Ho√†n th√†nh! ƒê·ªÉ s·ª≠ d·ª•ng:")
        print("1. S·ª≠a config/config.yml:")
        print(f"   llm_local_path: '{save_path}'")
        print("2. Ch·∫°y: python app.py")
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")

if __name__ == "__main__":
    download_small_model()